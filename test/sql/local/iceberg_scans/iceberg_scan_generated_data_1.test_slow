# name: test/sql/local/iceberg_scans/iceberg_scan_generated_data_1.test_slow
# description: test iceberg extension with the sf1 generated test set
# group: [iceberg_scans]

require avro

require parquet

require iceberg

require-env DUCKDB_ICEBERG_HAVE_GENERATED_DATA

# Define the iceberg table

statement ok
attach ':memory:' as my_datalake;

statement ok
create schema my_datalake.default;

statement ok
create view my_datalake.default.pyspark_iceberg_table_v1 as select * from ICEBERG_SCAN('__WORKING_DIRECTORY__/data/generated/iceberg/spark-local/default/pyspark_iceberg_table_v1');

# Define the intermediates table

statement ok
attach ':memory:' as intermediates;

statement ok
create schema intermediates.default;

statement ok
create view intermediates.default.pyspark_iceberg_table_v1 as select * from PARQUET_SCAN('__WORKING_DIRECTORY__/data/generated/intermediates/spark-local/pyspark_iceberg_table_v1/last/data.parquet/*.parquet');

# Check count matches
query I nosort count_iceberg_scan
SELECT count(*) FROM my_datalake.default.pyspark_iceberg_table_v1;

query I nosort count_iceberg_scan
select count(*) from intermediates.default.pyspark_iceberg_table_v1;

# Check data is identical, sorting by uuid to guarantee unique order.
query I nosort q1
SELECT COUNT(*) FROM my_datalake.default.pyspark_iceberg_table_v1;
----

query I nosort q1
SELECT COUNT(*) FROM intermediates.default.pyspark_iceberg_table_v1;
----

query I nosort q2
SELECT COUNT(*), MIN(l_suppkey_long), MAX(l_suppkey_long), SUM(l_suppkey_long) FROM my_datalake.default.pyspark_iceberg_table_v1;
----

query I nosort q2
SELECT COUNT(*), MIN(l_suppkey_long), MAX(l_suppkey_long), SUM(l_suppkey_long) FROM intermediates.default.pyspark_iceberg_table_v1;
----

# Full table compare: very slow
query I nosort q3
SELECT * FROM my_datalake.default.pyspark_iceberg_table_v1 WHERE uuid NOT NULL ORDER BY uuid;
----

query I nosort q3
SELECT * FROM intermediates.default.pyspark_iceberg_table_v1  WHERE uuid NOT NULL ORDER BY uuid;
----
